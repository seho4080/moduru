import json
import os
import time
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_PATH = os.path.join(BASE_DIR, "..", "..", "data", "raw", "vk_spot_data.json")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. í¬ë¡¬ ë“œë¼ì´ë²„ ì„¤ì •
options = Options()
# options.add_argument("--headless")
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
driver = webdriver.Chrome(service=Service(), options=options)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ìœ í‹¸ í•¨ìˆ˜
def get_text_or_none(tag):
    return tag.get_text(strip=True) if tag else None


def extract_images(soup):
    images = []
    for img in soup.select("div.photo_gallery div.swiper-slide img"):
        src = img.get("src") or img.get("data-src")
        if src and src.startswith("http"):
            images.append(src)
    return images


def extract_description(soup):
    desc = soup.select_one("div.wrap_contView > div.area_txtView.top p")
    return get_text_or_none(desc)


def extract_description_short(soup):
    tag = soup.select_one("div#topCp h3 em")
    return get_text_or_none(tag)


def extract_tags(soup):
    tags = []
    for span in soup.select("div.tag_cont span"):
        text = span.get_text(strip=True)
        if text.startswith("#"):
            text = text[1:]
        if text:
            tags.append(text)
    return tags


def extract_detail_info_for_spot(soup):
    info = {
        "info_center": None,
        "homepage": None,
        "address": None,
        "business_hours": None,
        "rest_date": None,
        "parking": None,
        "price": None,
    }

    parking_text = None
    parking_fee = None

    for li in soup.select("div#detailinfoview ul li"):
        strong = li.find("strong")
        spans = li.find_all("span")
        if not strong or not spans:
            continue

        label = strong.text.strip()
        values = [span.get_text(strip=True) for span in spans if span.text.strip()]

        if "ë¬¸ì˜" in label:
            info["info_center"] = values[-1]
        elif "í™ˆí˜ì´ì§€" in label:
            homepage = li.find("a")
            if homepage:
                info["homepage"] = homepage["href"]
        elif "ì£¼ì†Œ" in label:
            info["address"] = values[0]
        elif "ì´ìš©ì‹œê°„" in label or "ê´€ëŒì‹œê°„" in label:
            info["business_hours"] = "\n".join(values)
        elif "íœ´ì¼" in label:
            info["rest_date"] = values[0]
        elif label == "ì£¼ì°¨":
            parking_text = values[0]
        elif "ì£¼ì°¨ ìš”ê¸ˆ" in label:
            parking_fee = values[0]
        elif "ì…ì¥ë£Œ" in label or "ì´ìš©ìš”ê¸ˆ" in label:
            info["price"] = "\n".join(values)

    if parking_text and parking_fee:
        info["parking"] = f"{parking_text}\n{parking_fee}"
    elif parking_text:
        info["parking"] = parking_text
    elif parking_fee:
        info["parking"] = parking_fee

    return info


def crawl_spot(entry):
    url = entry["link"]
    name = entry.get("name", None)

    try:
        driver.get(url)
        time.sleep(0.5)
        soup = BeautifulSoup(driver.page_source, "html.parser")

        images = extract_images(soup)
        description = extract_description(soup)
        description_short = extract_description_short(soup)
        tags = extract_tags(soup)
        detail_info = extract_detail_info_for_spot(soup)

        # ê¸°ì¡´ entryì— ë®ì–´ì“°ê¸°
        entry.update(
            {
                "description": description,
                "description_short": description_short,
                "description_vector": None,
                "info_center": detail_info["info_center"],
                "homepage": detail_info["homepage"],
                "address": detail_info["address"],
                "business_hours": detail_info["business_hours"],
                "rest_date": detail_info["rest_date"],
                "parking": detail_info["parking"],
                "price": detail_info["price"],
                "images": images,
                "tags": tags,
            }
        )
        return entry

    except Exception as e:
        print(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {url} - {e}")
        return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ê¸°ì¡´ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
with open(DATA_PATH, "r", encoding="utf-8") as f:
    data = json.load(f)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. addressê°€ ì—†ëŠ” í•­ëª©ë§Œ ë‹¤ì‹œ ìˆ˜ì§‘
updated = 0
for idx, entry in enumerate(data):
    if entry.get("address") is not None:
        continue

    print(f"[{idx + 1}/{len(data)}] ğŸ“ Re-crawling: {entry.get('name')}")
    result = crawl_spot(entry)

    if result:
        data[idx] = result
        updated += 1
        with open(DATA_PATH, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… Updated: {entry.get('name')}")
    else:
        print(f"âŒ Failed: {entry.get('name')}")

print(f"\nğŸ” Done â€” {updated} spots updated with address info.")
driver.quit()
